{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# GPT4All in Python\n",
    "\n",
    "To run GPT4All on your computer, install the [nomic](https://home.nomic.ai) package:\n",
    "\n",
    "```bash\n",
    "pip install nomic\n",
    "```\n",
    "\n",
    "There are two ways to interact with GPT4All. You can simply call `gpt4all.prompt` to get a one-off response:\n",
    "or to have an extended conversation you can enter a context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load: ggml ctx size = 6065.35 MB\n",
      "llama_model_load: memory_size =  2048.00 MB, n_mem = 65536\n",
      "llama_model_load: loading model part 1/1 from '/Users/ben/.nomic/gpt4all-lora-quantized.bin'\n",
      "llama_model_load: .................................... done\n",
      "llama_model_load: model size =  4017.27 MB / num tensors = 291\n",
      "2023-03-30 10:26:05.079 | DEBUG    | nomic.gpt4all.gpt4all:__exit__:72 - Ending session...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Once upon a time there was a little brown rabbit named Bunnie Rabbit. She lived in the deepest part of the forest where no one ever went, except for her friend Bear. One day while she was out collecting berries and flowers from all over the woods, suddenly something grabbed hold of both arms! It turned out to be none other than Bear himself who had been watching Bunnie Rabbit's every move since they first met eachother in their childhood days when he saved her life. They became best friends ever after that day and have never parted ways till now, even though it has become a long time ago!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nomic.gpt4all as gpt4all\n",
    "\n",
    "gpt4all.prompt(\"Tell me a story about a bear who becomes friends with a bunny.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Running in a session\n",
    "\n",
    "A preferred way to run is inside a context manager. This allows a sustained conversion with the model that looks at\n",
    "previous responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load: ggml ctx size = 6065.35 MB\n",
      "llama_model_load: memory_size =  2048.00 MB, n_mem = 65536\n",
      "llama_model_load: loading model part 1/1 from '/Users/ben/.nomic/gpt4all-lora-quantized.bin'\n",
      "llama_model_load: .................................... done\n",
      "llama_model_load: model size =  4017.27 MB / num tensors = 291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------PROMPT-------\n",
      "Hello, there. I have a couple requests.\n",
      "-----RESPONSE------\n",
      "1) Is it possible for you or someone else on the team here at Stack Exchange Inc., to create a new tag called \"Asking Questions\" that would be used in conjunction with tags like [tag:programming] and [tag:mathematics]. This way, when people ask questions about asking good programming/mathematic related questions they can use this specific tag.\n",
      "2) Can you or someone else on the team here at Stack Exchange Inc., create a new feature that would allow users to search for tags based upon their own personal preferences? For example: I like coding and math, so if there is an article about programming in Python with some mathematical concepts incorporated into it, then this tag should appear.\n",
      "3) Can you or someone else on the team here at Stack Exchange Inc., create a new feature that would allow users to search for tags based upon their own personal preferences? For example: I like coding and math, so if there is an article about programming in Python with some mathematical concepts incorporated into it, then this tag should appear.\n",
      "------PROMPT-------\n",
      "First: tell me a joke.\n",
      "-----RESPONSE------\n",
      "2ndly : What's the difference between ignorance & stupidity ? Ignorant people don’t know they are being idiotic !!!!!!\n",
      "------PROMPT-------\n",
      "What's the largest city in the United States\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 10:26:52.338 | DEBUG    | nomic.gpt4all.gpt4all:__exit__:72 - Ending session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----RESPONSE------\n",
      "1 Answer is San Francisco, California which has an estimated population of over 873 thousand as per Census Bureau data.\n"
     ]
    }
   ],
   "source": [
    "with gpt4all.GPT4All() as session:\n",
    "    prompts = [\"Hello, there. I have a couple requests.\", \"First: tell me a joke.\", \"What's the largest city in the United States\"]\n",
    "    for prompt in prompts:\n",
    "        print(\"------PROMPT-------\\n\" + prompt)\n",
    "        response = session.prompt(prompt)\n",
    "        print(\"-----RESPONSE------\\n\" + response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also manually open and close a session. Be aware that a session uses a lot of memory, so you should be careful to \n",
    "close it when you are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load: ggml ctx size = 6065.35 MB\n",
      "llama_model_load: memory_size =  2048.00 MB, n_mem = 65536\n",
      "llama_model_load: loading model part 1/1 from '/Users/ben/.nomic/gpt4all-lora-quantized.bin'\n",
      "llama_model_load: .................................... done\n",
      "llama_model_load: model size =  4017.27 MB / num tensors = 291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I'm feeling great thanks for asking!\n",
      "2. Well it seems like there has been a lot of positive feedback on my work recently, so naturally I feel good about myself and what I do. It’s always nice to receive praise or recognition from your peers in any industry you are working within!\n"
     ]
    }
   ],
   "source": [
    "session = gpt4all.GPT4All()\n",
    "\n",
    "session.open()\n",
    "response = session.prompt(\"How are you doing today?\")\n",
    "print(response)\n",
    "response = session.prompt(\"Oh really? Why is that?\")\n",
    "print(response)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nomic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}