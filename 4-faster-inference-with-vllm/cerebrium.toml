[cerebrium.deployment]
name = "4-faster-inference-with-vllm"
include = ["./*", "main.py", "cerebrium.toml"]
exclude = [".*"]
shell_commands = []

# Note: Cuda 12 and Py 3.10 are required for VLLM
docker_base_image_url = "nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04"
python_version = "3.10"

[cerebrium.hardware]
cpu = 8
memory = 20.0
compute = "AMPERE_A10"
gpu_count = 1
provider = "aws"
region = "us-east-1"

[cerebrium.scaling]
min_replicas = 0
max_replicas = 2
cooldown = 10

[cerebrium.dependencies.pip]
sentencepiece = "latest"
torch = "latest"
vllm = "latest"
transformers = "latest"
accelerate = "latest"
xformers = "latest"

[cerebrium.dependencies.apt]
git = "latest"