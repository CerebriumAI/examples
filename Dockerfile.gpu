# Stage 1: Build llama.cpp server-cuda
FROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS builder  # CUDA & cuDNN support :contentReference[oaicite:12]{index=12}

RUN apt-get update && apt-get install -y \
      cmake \
      build-essential \
      git \
      pkg-config \
      libssl-dev \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /build
RUN git clone https://github.com/ggml-org/llama.cpp.git . && \
    cmake -B build -DGGML_CUDA=ON -DLLAMA_CUBLAS=ON -DCMAKE_CUDA_COMPILER=$(which nvcc) . && \
    cmake --build build --target server-cuda -j$(nproc)

# Stage 2: Final Orpheus-FastAPI image
FROM ubuntu:22.04
ENV DEBIAN_FRONTEND=noninteractive

# (Or optionally start again FROM a smaller Python base...)
RUN apt-get update && apt-get install -y python3.10 python3-pip libsndfile1 ffmpeg portaudio19-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy only the server binary from builder
COPY --from=builder /build/build/server-cuda /usr/local/bin/llama-server

# Then set up your Python venv, install torch & Orpheus dependenciesâ€¦
COPY requirements.txt .
RUN python3 -m venv /venv && \
    /venv/bin/pip install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 \
    && /venv/bin/pip install --no-cache-dir -r requirements.txt

# Expose your FastAPI port and entrypoint
EXPOSE 5005
CMD ["/venv/bin/uvicorn", "app:app", "--host", "0.0.0.0", "--port", "5005"]
