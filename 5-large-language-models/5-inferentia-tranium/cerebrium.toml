
[cerebrium.deployment]
name = "llama"
python_version = "3.10"
include = ["./*", "main.py", "cerebrium.toml"]
exclude = ["./example_exclude"]
docker_base_image_url = "public.ecr.aws/neuron/pytorch-inference-neuronx:2.5.1-neuronx-py310-sdk2.21.1-ubuntu22.04"
pre_build_commands = [
    "pip install transformers-neuronx torch-neuronx neuronx-cc --extra-index-url=https://pip.repos.neuron.amazonaws.com",
    "pip install torch_xla"
]

[cerebrium.hardware]
region = "us-east-1"
provider = "aws"
compute = "TRN1"
cpu = 42
memory = 330.0
gpu_count = 16

[cerebrium.scaling]
min_replicas = 1 # Note: This incurs a constant cost since at least one instance is always running.
max_replicas = 5
cooldown = 60
response_grace_period = 2700 

[cerebrium.dependencies.pip]
torch = ">=2.0.0"
sentencepiece = "latest"
transformers = "latest"
huggingface_hub = "latest"
pydantic = "latest"
vllm = "latest"

[cerebrium.dependencies.conda]

[cerebrium.dependencies.apt]
